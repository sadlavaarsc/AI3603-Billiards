架构设计：
采用Alpha-Zero算法，包括以下组件：
1.行为网络：
行为网络输入连续三局的状态，学习并输出赢家在当前状态下的最优行为（单动作向量）。
2.价值网络：
价值网络输入连续三局的状态，学会分类胜局和败局，然后选择胜局的概率作为胜率
3.蒙特卡洛树搜索：
对于每回合，使用行为网络直接计算当前状态下的最优行为，然后在最优行为的基础上搜索相似的动作，对于每个动作模拟结果，并根据价值网络判断胜局和败局，计算胜率，随后根据蒙特卡洛树搜索的规则正确更新决策树，直到输出结果。
4.闭环训练：
训练初期直接使用Basic Agent生成的一万条对局数据，经过处理后分别训练行为网络和价值网络，
随后完成完整系统，使用全新的Agent替代Basic Agent，生成更多对局数据，用于训练行为网络和价值网络，
不断迭代优化，直到达到预期的性能。