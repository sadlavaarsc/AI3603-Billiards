import os
import json
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import gc
import shutil
import time
import sys

# ä¿®å¤Windowsè·¯å¾„é—®é¢˜çš„æ ¸å¿ƒå·¥å…·å‡½æ•°


def get_absolute_path(path):
    """è·å–ç»å¯¹è·¯å¾„ï¼Œå¤„ç†Windowsè·¯å¾„åˆ†éš”ç¬¦"""
    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    abs_path = os.path.abspath(path)
    # å¤„ç†Windowsè·¯å¾„åˆ†éš”ç¬¦
    abs_path = abs_path.replace('/', '\\')
    return abs_path


def ensure_dir_exists(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆå¤„ç†æ–‡ä»¶è·¯å¾„å’Œç›®å½•è·¯å¾„ä¸¤ç§æƒ…å†µï¼‰"""
    # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œå–å…¶ç›®å½•
    if os.path.splitext(path)[1] != '':
        dir_path = os.path.dirname(path)
    else:
        dir_path = path

    # å¦‚æœç›®å½•ä¸ºç©ºï¼ˆå½“å‰ç›®å½•ï¼‰ï¼Œç›´æ¥è¿”å›
    if not dir_path:
        return True

    # åˆ›å»ºç›®å½•ï¼ˆé€’å½’åˆ›å»ºå¤šçº§ç›®å½•ï¼‰
    try:
        os.makedirs(dir_path, exist_ok=True)
        return True
    except Exception as e:
        print(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {dir_path} | é”™è¯¯: {str(e)[:100]}")
        return False

# æ¨¡æ‹Ÿä¾èµ–æ¨¡å—


def process_match_data(match_dir, output_file):
    """æ¨¡æ‹Ÿç”Ÿæˆè®­ç»ƒæ•°æ®"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    ensure_dir_exists(output_file)
    with open(output_file, 'w', encoding='utf-8') as f:
        samples = []
        for i in range(10):
            samples.append({
                "states": [[float(i)]*81, [float(i+1)]*81, [float(i+2)]*81],
                "action": [0.1*i, 0.2*i, 0.3*i, 0.4*i, 0.5*i],
                "value": 0.8*i
            })
        json.dump(samples, f, ensure_ascii=False)


class DualNetwork(nn.Module):
    """æ¨¡æ‹ŸåŒç½‘ç»œæ¨¡å‹"""

    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(3*81, 128)
        self.policy_head = nn.Linear(128, 5)
        self.value_head = nn.Linear(128, 1)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc(x))
        return {
            'policy_output': self.policy_head(x),
            'value_output': self.value_head(x)
        }

# ========== æ ¸å¿ƒä¿®å¤ï¼šWindowsè·¯å¾„å…¼å®¹çš„æµå¼æ•°æ®é›† ==========


class StreamingBilliardsDataset(Dataset):
    def __init__(self, json_file, preprocessor=None, use_existing_index=True):
        # æ ¸å¿ƒä¿®å¤1ï¼šç»Ÿä¸€è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        self.json_file = get_absolute_path(json_file)
        self.preprocessor = preprocessor
        self.sample_indices = []

        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.json_file):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.json_file}")
            self.file_size = 0
        else:
            self.file_size = os.path.getsize(self.json_file)

        # æ ¸å¿ƒä¿®å¤2ï¼šç´¢å¼•æ–‡ä»¶ä½¿ç”¨ç»å¯¹è·¯å¾„
        self.index_file = get_absolute_path(f"{self.json_file}.index.json")
        self.use_existing_index = use_existing_index
        self.chunk_size = min(512 * 1024 * 1024, self.file_size //
                              4 if self.file_size > 0 else 512 * 1024 * 1024)

        # é¢„æ‰«ææ–‡ä»¶
        self._scan_sample_positions()

    def _load_index_file(self):
        """åŠ è½½ç´¢å¼•æ–‡ä»¶ï¼ˆWindowsè·¯å¾„å…¼å®¹ï¼‰"""
        try:
            if not os.path.exists(self.index_file):
                return False

            # éªŒè¯æ–‡ä»¶å¯è¯»å–
            if not os.access(self.index_file, os.R_OK):
                print(f"âŒ æ— è¯»å–æƒé™: {self.index_file}")
                return False

            # æµå¼è¯»å–JSON
            with open(self.index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)

            # æ ¡éªŒ
            current_file_size = os.path.getsize(
                self.json_file) if os.path.exists(self.json_file) else 0
            if index_data.get('file_size', 0) != current_file_size:
                print(f"âš ï¸  ç´¢å¼•ä¸æ•°æ®æ–‡ä»¶å¤§å°ä¸åŒ¹é…ï¼Œé‡æ–°ç”Ÿæˆ...")
                return False

            self.sample_indices = index_data.get('sample_indices', [])
            if len(self.sample_indices) == 0:
                print(f"âš ï¸  ç´¢å¼•æ–‡ä»¶æ— æœ‰æ•ˆæ•°æ®ï¼Œé‡æ–°ç”Ÿæˆ...")
                return False

            print(f"âœ… æˆåŠŸåŠ è½½ç´¢å¼•: {self.index_file}")
            print(f"   æ ·æœ¬æ•°: {len(self.sample_indices)}")
            return True

        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç´¢å¼•å¤±è´¥: {str(e)[:100]}ï¼Œé‡æ–°ç”Ÿæˆ...")
            return False

    def _save_index_file(self):
        """ä¿å­˜ç´¢å¼•æ–‡ä»¶ï¼ˆWindowsè·¯å¾„æ ¸å¿ƒä¿®å¤ï¼‰"""
        try:
            # æ ¸å¿ƒä¿®å¤3ï¼šå¼ºåˆ¶ç¡®ä¿ç´¢å¼•æ–‡ä»¶ç›®å½•å­˜åœ¨
            if not ensure_dir_exists(self.index_file):
                return False

            # éªŒè¯ç›®å½•å¯å†™å…¥
            index_dir = os.path.dirname(self.index_file)
            if not os.access(index_dir, os.W_OK):
                print(f"âŒ æ— å†™å…¥æƒé™: {index_dir}")
                return False

            # å‡†å¤‡ç´¢å¼•æ•°æ®
            index_data = {
                'version': '1.0',
                'file_size': self.file_size,
                'create_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_samples': len(self.sample_indices),
                'sample_indices': self.sample_indices,
                'json_file_path': self.json_file,
                'index_file_path': self.index_file
            }

            # æ ¸å¿ƒä¿®å¤4ï¼šWindowsä¸‹ä½¿ç”¨æ­£ç¡®çš„å†™å…¥æ–¹å¼
            with open(self.index_file, 'w', encoding='utf-8', newline='') as f:
                # æ ¼å¼åŒ–JSONï¼Œä¾¿äºæŸ¥çœ‹
                json_str = json.dumps(index_data, ensure_ascii=False, indent=2)
                # åˆ†å—å†™å…¥ï¼Œé¿å…å¤§æ–‡ä»¶é—®é¢˜
                chunk_size = 4096
                for i in range(0, len(json_str), chunk_size):
                    f.write(json_str[i:i+chunk_size])
                f.flush()  # å¼ºåˆ¶åˆ·ç›˜
                os.fsync(f.fileno())  # Windowsä¸‹å¼ºåˆ¶å†™å…¥ç£ç›˜

            # æœ€ç»ˆéªŒè¯
            if not os.path.exists(self.index_file):
                print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¿å­˜åä¸å­˜åœ¨: {self.index_file}")
                return False

            file_size = os.path.getsize(self.index_file)
            print(f"âœ… ç´¢å¼•ä¿å­˜æˆåŠŸ: {self.index_file}")
            print(f"   å¤§å°: {file_size / 1024:.2f} KB")
            return True

        except PermissionError:
            print(f"âŒ ä¿å­˜å¤±è´¥ï¼šæ— å†™å…¥æƒé™ï¼Œè¯·ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ")
            return False
        except Exception as e:
            print(f"âŒ ä¿å­˜ç´¢å¼•å¤±è´¥: {str(e)[:100]}")
            # æ¸…ç†æ®‹ç¼ºæ–‡ä»¶
            if os.path.exists(self.index_file):
                try:
                    os.remove(self.index_file)
                except:
                    pass
            return False

    def _scan_sample_positions(self):
        """æ‰«ææ ·æœ¬ä½ç½®ï¼ˆWindowså…¼å®¹ï¼‰"""
        # ä¼˜å…ˆåŠ è½½ç´¢å¼•
        if self.use_existing_index and self._load_index_file():
            return

        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        if not os.path.exists(self.json_file):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ‰«æ")
            return
        if self.file_size == 0:
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡æ‰«æ")
            return

        print(f"\nğŸ“Š æ‰«ææ•°æ®æ–‡ä»¶: {self.json_file}")
        print(f"   å¤§å°: {self.file_size / (1024*1024):.2f} MB")
        print(f"   å—å¤§å°: {self.chunk_size / (1024*1024):.2f} MB")

        sample_indices = []
        depth = 0
        current_pos = 0

        # äºŒè¿›åˆ¶æ¨¡å¼è¯»å–ï¼Œé¿å…ç¼–ç é—®é¢˜
        with open(self.json_file, 'rb') as f:
            # è·³è¿‡å¼€å¤´çš„[
            while True:
                byte = f.read(1)
                current_pos += 1
                if not byte:
                    break
                char = byte.decode('utf-8', errors='ignore')
                if char == '[':
                    break
                if char not in [' ', '\n', '\r', '\t']:
                    print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸æ˜¯JSONæ•°ç»„")
                    return

            # è¿›åº¦æ¡
            pbar = tqdm(total=self.file_size, desc="è§£ææ ·æœ¬ä½ç½®",
                        unit="B", unit_scale=True)
            pbar.update(current_pos)

            # åˆ†å—è§£æ
            buffer = b''
            while True:
                chunk = f.read(self.chunk_size)
                if not chunk:
                    break
                buffer += chunk
                buffer_pos = 0
                buffer_len = len(buffer)

                while buffer_pos < buffer_len:
                    byte = buffer[buffer_pos:buffer_pos+1]
                    try:
                        char = byte.decode('utf-8')
                    except:
                        char = ''
                    buffer_pos += 1
                    current_pos += 1

                    if current_pos % 1000 == 0:
                        pbar.update(1000)

                    if char in [' ', '\n', '\r', '\t', ',']:
                        continue

                    if char == '{':
                        depth += 1
                        if depth == 1:
                            sample_start = current_pos - \
                                1 - len(buffer) + buffer_pos
                            sample_indices.append(sample_start)
                    elif char == '}':
                        depth -= 1

                buffer = buffer[-1000:] if buffer_len > 1000 else b''

            pbar.update(self.file_size - pbar.n)
            pbar.close()

        self.sample_indices = sample_indices
        print(f"\nâœ… æ‰«æå®Œæˆï¼æ ·æœ¬æ•°: {len(sample_indices)}")

        # ä¿å­˜ç´¢å¼•
        self._save_index_file()

    def __len__(self):
        return len(self.sample_indices)

    def __getitem__(self, idx):
        """è¯»å–æ ·æœ¬ï¼ˆWindowså…¼å®¹ï¼‰"""
        try:
            if idx < 0 or idx >= len(self.sample_indices):
                raise IndexError(f"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´")

            start_pos = self.sample_indices[idx]

            with open(self.json_file, 'r', encoding='utf-8') as f:
                f.seek(start_pos)
                sample_str = ''
                depth = 0
                while True:
                    char = f.read(1)
                    if not char:
                        break
                    sample_str += char

                    if char == '{':
                        depth += 1
                    elif char == '}':
                        depth -= 1
                        if depth == 0:
                            break

                sample = json.loads(sample_str)

            states = np.array(sample['states'], dtype=np.float32)
            action = np.array(sample['action'], dtype=np.float32)
            value = np.array([sample['value']], dtype=np.float32)

            if self.preprocessor is not None:
                states = self.preprocessor(states)

            return (
                torch.from_numpy(states),
                torch.from_numpy(action),
                torch.from_numpy(value)
            )

        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ ·æœ¬ {idx} å¤±è´¥: {str(e)[:100]}")
            return (
                torch.zeros(3, 81, dtype=torch.float32),
                torch.zeros(5, dtype=torch.float32),
                torch.zeros(1, dtype=torch.float32)
            )

# ========== çŠ¶æ€é¢„å¤„ç†å™¨ ==========


class StatePreprocessor:
    def __call__(self, states):
        states[:, 64] = states[:, 64] / 2.540
        states[:, 65] = states[:, 65] / 2.540
        states[:, :64:4] = states[:, :64:4] / 2.540
        states[:, 1:64:4] = states[:, 1:64:4] / 1.270
        return states

# ========== è®­ç»ƒå‡½æ•° ==========


def train(args):
    """è®­ç»ƒå‡½æ•°ï¼ˆWindowsè·¯å¾„å…¼å®¹ï¼‰"""
    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    args.train_data_file = get_absolute_path(args.train_data_file)
    args.model_dir = get_absolute_path(args.model_dir)
    args.match_dir = get_absolute_path(args.match_dir)

    # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
    ensure_dir_exists(args.model_dir)

    # æ•°æ®å¤„ç†
    if args.use_existing_data:
        if os.path.exists(args.train_data_file):
            print(f"âœ… ä½¿ç”¨å·²æœ‰æ•°æ®: {args.train_data_file}")
        else:
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.train_data_file}")
            if args.auto_generate_if_missing:
                print(f"ğŸ”„ è‡ªåŠ¨ç”Ÿæˆæ•°æ®...")
                process_match_data(args.match_dir, args.train_data_file)
            else:
                return
    else:
        print(f"ğŸ”„ é‡æ–°ç”Ÿæˆæ•°æ®...")
        process_match_data(args.match_dir, args.train_data_file)
        # åˆ é™¤æ—§ç´¢å¼•
        index_file = get_absolute_path(f"{args.train_data_file}.index.json")
        if os.path.exists(index_file):
            try:
                os.remove(index_file)
                print(f"ğŸ—‘ï¸ åˆ é™¤æ—§ç´¢å¼•: {index_file}")
            except:
                print(f"âš ï¸ åˆ é™¤æ—§ç´¢å¼•å¤±è´¥")

    gc.collect()

    # åŠ è½½æ•°æ®é›†
    print("\nğŸ“¥ åŠ è½½æ•°æ®é›†...")
    preprocessor = StatePreprocessor()
    dataset = StreamingBilliardsDataset(
        json_file=args.train_data_file,
        preprocessor=preprocessor,
        use_existing_index=args.use_existing_index
    )

    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0,  # Windowsä¸‹å¿…é¡»è®¾ä¸º0
        pin_memory=True if torch.cuda.is_available() else False,
        drop_last=True
    )

    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"   æ€»æ ·æœ¬: {len(dataset)}")
    print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"   æ€»æ‰¹æ¬¡: {len(dataloader)}")

    # åˆå§‹åŒ–æ¨¡å‹
    print("\nğŸ”§ åˆå§‹åŒ–æ¨¡å‹...")
    model = DualNetwork()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    print(f"ğŸ’» è®¾å¤‡: {device}")

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    policy_criterion = nn.MSELoss()
    value_criterion = nn.MSELoss()
    optimizer = optim.Adam(
        model.parameters(),
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )
    scheduler = optim.lr_scheduler.StepLR(
        optimizer,
        step_size=args.lr_step_size,
        gamma=args.lr_gamma
    )

    # è®­ç»ƒå¾ªç¯
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None

    for epoch in range(args.epochs):
        model.train()
        total_policy_loss = 0.0
        total_value_loss = 0.0
        total_loss = 0.0

        with tqdm(dataloader, desc=f"Epoch {epoch+1}/{args.epochs}") as pbar:
            for batch_idx, (states, policy_targets, value_targets) in enumerate(pbar):
                states = states.to(device, non_blocking=True)
                policy_targets = policy_targets.to(device, non_blocking=True)
                value_targets = value_targets.to(device, non_blocking=True)

                if scaler:
                    with torch.cuda.amp.autocast():
                        outputs = model(states)
                        policy_loss = policy_criterion(
                            outputs['policy_output'], policy_targets)
                        value_loss = value_criterion(
                            outputs['value_output'], value_targets)
                        loss = policy_loss + value_loss
                else:
                    outputs = model(states)
                    policy_loss = policy_criterion(
                        outputs['policy_output'], policy_targets)
                    value_loss = value_criterion(
                        outputs['value_output'], value_targets)
                    loss = policy_loss + value_loss

                optimizer.zero_grad()
                if scaler:
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    loss.backward()
                    optimizer.step()

                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                total_loss += loss.item()

                pbar.set_postfix({
                    'Policy Loss': f'{policy_loss.item():.6f}',
                    'Value Loss': f'{value_loss.item():.6f}',
                    'Total Loss': f'{loss.item():.6f}'
                })

                if batch_idx % 100 == 0:
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

        scheduler.step()

        avg_policy_loss = total_policy_loss / len(dataloader)
        avg_value_loss = total_value_loss / len(dataloader)
        avg_total_loss = total_loss / len(dataloader)

        print(f"\nğŸ“ˆ Epoch {epoch+1} ç»“æœ:")
        print(f"   å¹³å‡ç­–ç•¥æŸå¤±: {avg_policy_loss:.6f}")
        print(f"   å¹³å‡ä»·å€¼æŸå¤±: {avg_value_loss:.6f}")
        print(f"   å¹³å‡æ€»æŸå¤±: {avg_total_loss:.6f}")
        print(f"   å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6e}")

        # ä¿å­˜æ¨¡å‹
        if (epoch + 1) % args.save_interval == 0 or (epoch + 1) == args.epochs:
            checkpoint_path = os.path.join(
                args.model_dir, f"dual_network_epoch_{epoch+1}.pt")
            torch.save({
                'epoch': epoch+1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_total_loss,
            }, checkpoint_path)
            print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜: {checkpoint_path}")

        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(args.model_dir, "dual_network_final.pt")
    torch.save(model.state_dict(), final_model_path)
    print(f"\nğŸ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹: {final_model_path}")

# ========== ä¸»å‡½æ•° ==========


def main():
    parser = argparse.ArgumentParser(description="å°çƒåŒç½‘ç»œè®­ç»ƒï¼ˆWindowså…¼å®¹ç‰ˆï¼‰")

    # æ ¸å¿ƒå‚æ•°
    parser.add_argument('--use_existing_data', action='store_true',
                        help='ä½¿ç”¨å·²æœ‰è®­ç»ƒæ•°æ®')
    parser.add_argument('--auto_generate_if_missing', action='store_true',
                        help='æ•°æ®ç¼ºå¤±æ—¶è‡ªåŠ¨ç”Ÿæˆ')
    parser.add_argument('--use_existing_index', action='store_true', default=True,
                        help='ä½¿ç”¨å·²æœ‰ç´¢å¼•æ–‡ä»¶')

    # è·¯å¾„å‚æ•°
    parser.add_argument('--match_dir', type=str, default='match_data',
                        help='åŸå§‹å¯¹å±€æ•°æ®ç›®å½•')
    parser.add_argument('--train_data_file', type=str, default='trainable_data.json',
                        help='è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model_dir', type=str, default='models',
                        help='æ¨¡å‹ä¿å­˜ç›®å½•')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=3,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=4,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                        help='æƒé‡è¡°å‡')
    parser.add_argument('--lr_step_size', type=int, default=10,
                        help='å­¦ä¹ ç‡è¡°å‡æ­¥é•¿')
    parser.add_argument('--lr_gamma', type=float, default=0.5,
                        help='å­¦ä¹ ç‡è¡°å‡ç³»æ•°')
    parser.add_argument('--save_interval', type=int, default=1,
                        help='æ¨¡å‹ä¿å­˜é—´éš”')

    args = parser.parse_args()

    # Windowsä¸‹çš„é¢å¤–æ£€æŸ¥
    if sys.platform == 'win32':
        print(f"ğŸ” Windowsç³»ç»Ÿæ£€æµ‹ï¼Œè‡ªåŠ¨å¤„ç†è·¯å¾„é—®é¢˜")
        # ç¡®ä¿å½“å‰ç›®å½•å¯å†™
        if not os.access('.', os.W_OK):
            print(f"âš ï¸ å½“å‰ç›®å½•æ— å†™å…¥æƒé™ï¼Œè¯·ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ")

    train(args)


if __name__ == '__main__':
    main()
