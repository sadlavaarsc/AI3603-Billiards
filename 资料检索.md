这套架构和流程在**强化学习与博弈AI领域**有明确的学术名称，核心关键词可分为 **核心架构名**、**关键技术术语** 两类，方便沟通、检索文献：

### 一、 核心架构名称
1.  **AlphaZero 架构**
    DeepMind 在 **《Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm》** 中提出的 AlphaZero 核心框架，对应关系如下：
    | 方案模块 | AlphaZero 核心模块 |
    |--------------|---------------------|
    | 行为网络 + 胜率网络 | 策略-价值双网络（Policy-Value Network） |
    | 基于双网络指导的 MCTS | 神经网络增强型蒙特卡洛树搜索（Neural Network-Augmented MCTS） |
    | 监督预训练 → 自对弈 → 迭代优化 | 监督预训练（可选）+ 自对弈强化学习闭环 |

    AlphaZero 是该类架构的**标杆**，检索这个关键词能找到最权威的论文、实现教程和开源项目。

2.  **通用博弈强化学习架构（General Game Playing RL Architecture）**
    适合更宽泛的描述，强调该架构可迁移到围棋、象棋、台球等各类回合制博弈场景。

### 二、 关键技术术语（拆解架构细节，辅助理解）
1.  **策略-价值双网络（Policy-Value Dual Network）**
    「行为网络」对应学术中的 **策略网络（Policy Network）**，作用是输出动作的概率分布；「胜率网络」对应 **价值网络（Value Network）**，作用是评估当前局面的价值（胜率）。双网络共享特征提取层是 AlphaZero 的核心创新之一。

2.  **自对弈强化学习（Self-Play Reinforcement Learning）**
    这是整个闭环的核心训练范式，指 AI 通过与自身迭代后的版本对抗，生成高质量训练数据，实现能力的自我进化。

3.  **蒙特卡洛树搜索增强（MCTS Enhancement）**
    区别于传统无指导的 MCTS，此方案中 MCTS 由双网络提供**动作先验概率**和**节点价值评估**，属于「有监督的 MCTS」，学术上称为 **Neural MCTS** 或 **Guided MCTS**。

4.  **经验回放缓冲区（Replay Buffer）**
    对应存储自对弈数据的模块，是强化学习中稳定训练的关键技术，用于避免模型过拟合到最新数据。

### 三、 检索建议
1.  优先搜索 **AlphaZero 论文 + 开源实现**，直接看 DeepMind 原版论文和社区复现项目（如 GitHub 上的 `alpha-zero-general`），快速理解架构原理；
2.  若聚焦台球等非完全信息博弈，可补充关键词 **AlphaZero for Billiards** 或 **MCTS-Policy-Value Network in Turn-Based Games**；
3.  中文资料可搜索 **策略价值双网络 + 自对弈 MCTS**，找到国内学者的解读和应用案例。
