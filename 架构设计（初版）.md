# 基于MCTS+双网络的台球AI系统架构设计文档（适配随机性环境）

## 文档信息

|项目|内容|
|---|---|
|项目名称|基于MCTS+双网络的台球AI系统|
|开发周期|1个月（4周）|
|开发人数|建议5人分工：1名算法架构师、2名模型开发工程师、1名仿真环境开发工程师、1名测试验证工程师|
|核心目标|1. 超越贝叶斯硬AI，对贝叶斯AI胜率≥70%；2. 适配环境随机性，策略稳健性≥85%（相同局面最优动作选择一致性）；3. 实时决策要求：单次决策≤1秒；4. 模型规模约束：总参数≤2000万（满足实时推理）|
|关键约束|台球仿真环境存在随机性（相同输入动作可能产生微小局面偏差），需保障MCTS决策可靠性|
|技术基线|Python 3.9 + PyTorch 2.0 + PoolTool + Numba加速|
## 一、整体架构设计（分层+抗随机增强）

### 1.1 核心架构总览

采用“感知层-决策层-执行层-数据层”四层架构，核心创新点为“双网络+抗随机MCTS决策系统”，通过“多轨迹模拟、不确定性建模、稳健性训练”三重机制适配环境随机性，整体逻辑对齐AlphaGo核心范式并针对性优化台球场景特性；其中执行层基于PoolTool实现仿真环境交互，替代原Gym依赖：

```mermaid
 B[决策层：抗随机MCTS+双网络]
    B --> C[执行层：基于PoolTool的随机仿真环境交互]
    C --> D[数据层：轨迹存储与回流]
    D --> B
    
    subgraph 感知层
    A1[原始局面输入：球坐标/球权/剩余杆数/已进袋列表] --> A2[特征预处理：归一化+随机扰动增强] --> A3[高阶表征提取：轻量ResNet]
    end
    
    subgraph 决策层
    B1[行为网络：生成高质量候选动作]
    B2[价值网络：预测胜率+方差（不确定性）]
    B3[抗随机MCTS核心：多轨迹模拟+方差惩罚UCB]
    B1 --> B3
    B2 --> B3
    end
    
    subgraph 执行层
    C1[PoolTool封装随机仿真环境]
    C2[规则引擎：合法/犯规判定（适配PoolTool输出）]
    C3[奖励计算：含稳健性奖励项]
    C4[多轨迹执行器：同一动作多次执行]
    end
    
    subgraph 数据层
    D1[预训练数据集：贝叶斯AI轨迹+人类标注]
    D2[自对弈轨迹池：含多轨迹扰动数据]
    D3[模型更新模块：TD学习+策略梯度+不确定性校准]
    end
    ">chart
```
### 1.2 模块分工与核心职责

|模块|核心职责|开发负责人|关键交付物|验收标准|
|---|---|---|---|---|
|感知层|1. 局面特征提取与编码；2. 数据增强（含随机扰动）；3. 高阶表征生成|算法架构师+模型开发1|特征编码函数、表征网络代码、数据增强脚本|特征维度≤56维，表征网络推理时间≤20ms|
|行为网络|1. 生成合法候选动作（角度/力度/击球点）；2. 输出动作先验概率；3. 过滤高风险非法动作|模型开发1|行为网络训练/推理代码、动作过滤模块、预训练+微调权重|动作预测准确率≥80%，候选动作非法率≤3%，推理时间≤50ms|
|价值网络|1. 预测局面综合价值（胜率）；2. 输出价值方差（不确定性）；3. 支持己方视角适配|模型开发2|价值网络训练/推理代码、不确定性校准模块、预训练+微调权重|胜率预测MSE≤0.01，方差预测与实际偏差≤0.05，推理时间≤50ms|
|抗随机MCTS|1. 多轨迹模拟抵消随机性；2. 方差惩罚UCB公式选动作；3. 动态调整搜索参数；4. Numba加速核心逻辑|算法架构师+模型开发1|MCTS核心代码（Numba加速版）、决策接口、参数配置文件|单次决策时间≤1s，策略稳健性≥85%，关键局面决策正确率≥90%|
|随机仿真环境|1. 基于PoolTool实现动作执行与随机偏差模拟；2. 规则判定（合法/犯规/终局，适配PoolTool输出格式）；3. 多轨迹执行支持；4. 标准化接口封装（适配决策层调用）|仿真环境开发|PoolTool封装环境代码、规则引擎（适配PoolTool）、多轨迹执行器、环境测试用例|规则判定准确率≥99%，相同动作随机偏差符合真实台球物理规律|
|训练闭环|1. 预训练数据处理；2. 自对弈轨迹生成；3. 双网络更新（含不确定性校准）；4. 模型版本管理|模型开发2+算法架构师|训练脚本、数据管理工具、模型更新日志、权重文件|预训练收敛epoch≤100，自对弈10万轮后胜率≥70%|
|测试验证|1. 胜率测试（vs贝叶斯AI）；2. 实时性测试；3. 稳健性测试；4. 规则适配性测试|测试验证工程师|测试用例、性能报告、稳健性分析报告|对贝叶斯AI胜率≥70%，单次决策≤1s，非法动作率≤5%|
## 二、技术栈选型（含抗随机优化组件）

### 2.1 核心技术栈

|技术领域|选型|选型理由|核心作用|
|---|---|---|---|
|深度学习框架|PyTorch 2.0+|动态图调试友好，支持GPU加速，适配强化学习动态训练场景|双网络训练与推理，支持TorchScript编译优化|
|仿真环境|PoolTool + 自定义规则适配层|PoolTool具备精准的台球物理模拟能力，支持自定义动作输入与局面输出；自定义规则适配层可将PoolTool输出转换为决策层所需格式，适配贝叶斯AI交互场景|基于PoolTool的动作执行、随机偏差生成、规则判定，支持贝叶斯AI交互模拟|
|MCTS实现|基于alpha-mcts改造 + Numba 0.57.1加速|复用AlphaGo风格MCTS逻辑，Numba可将Python核心函数编译为机器码，提升回溯效率10-20倍|抗随机MCTS决策，支持多轨迹模拟与方差惩罚|
|特征处理|NumPy 1.24.3 + Pandas 2.0.2|高效处理数值特征，支持批量归一化与随机扰动|局面特征提取、归一化、数据增强|
|性能加速|Numba（MCTS核心） + PyTorch GPU推理（双网络）|小模型GPU推理比CPU快5-10倍，Numba解决Python回溯效率低问题|保障实时决策要求（≤1s）|
|数据管理|SQLite + Pickle|轻量化存储，适配小批量自对弈轨迹（10万轮级），开发成本低|预训练数据、自对弈轨迹的存储与读取|
|可视化与分析|Matplotlib 3.7.1 + Seaborn 0.12.2|简洁易用，支持训练曲线、MCTS搜索过程、胜率分布可视化|训练监控、稳健性分析、测试结果展示|
|版本控制|Git + GitLab/GitHub|多人协作代码管理，支持版本回溯与分支开发|代码协同、模型版本管理、开发进度跟踪|
### 2.2 环境依赖清单（精确版本）

```Plain Text

# 核心依赖（必须严格匹配版本，避免兼容性问题）
python==3.9.16
torch==2.0.1+cu118  # 支持GPU加速，无GPU可改为torch==2.0.1+cpu
pooltool==3.5.0  # 台球物理模拟核心依赖
numpy==1.24.3
numba==0.57.1
scipy==1.10.1
pandas==2.0.2
matplotlib==3.7.1
seaborn==0.12.2
sqlite3==3.41.2
pickle-mixin==1.0.2
pybind11==2.10.4  # 可选，C++加速MCTS核心用（若Numba加速不满足）
torchscript==2.0.1  # 模型推理优化
```

## 三、核心模块详细设计（含抗随机机制+精确参数）

### 3.1 感知层：局面特征设计与编码

#### 3.1.1 输入特征明细（适配随机性环境增强）

|特征类型|具体内容|维度|编码/处理方式|抗随机增强措施|
|---|---|---|---|---|
|离散特征|球权（己方/对方）、己方目标球类型（实心/条纹）、剩余杆数（0-60）、已进袋球列表（16颗球）|1+1+1+16=19|球权/目标球类型：Embedding（维度16）；剩余杆数：归一化到[0,1]；已进袋列表：独热编码|无（离散特征无随机性）|
|连续特征|16颗球的(x,y)坐标（母球+15颗目标球，单位cm）、母球击球点可选范围（x/y方向）|16×2 + 2=34|归一化到[0,1]（以台球桌尺寸为基准：标准桌284×152.5cm）|数据增强时添加±1cm随机扰动，模拟环境随机性|
|先验特征|贝叶斯AI输出的合法动作掩码、进袋概率、犯规风险|3|归一化到[0,1]|融合多组贝叶斯AI输出的均值，降低单一预测偏差|
|总计|-|19+34+3=56|-|-|
#### 3.1.2 表征网络设计（轻量型，精确参数）

核心目标：高效提取局面高阶特征，参数规模可控，适配实时推理

|模块|结构配置|参数规模|核心作用|
|---|---|---|---|
|输入层|接收56维融合特征，输出维度56|0|特征输入适配|
|残差块1|1×1卷积（输入56→输出64）+ ReLU + 批量归一化； shortcut连接（1×1卷积适配维度）|56×64 + 64×64=7040|特征升维与初步提取|
|残差块2|1×1卷积（输入64→输出128）+ ReLU + 批量归一化； shortcut连接|64×128 + 128×128=24576|特征深度提取|
|残差块3|1×1卷积（输入128→输出128）+ ReLU + 批量归一化； shortcut连接|128×128 + 128×128=32768|特征强化与稳定|
|残差块4|1×1卷积（输入128→输出256）+ ReLU + 批量归一化； shortcut连接|128×256 + 256×256=98304|高阶特征输出|
|输出层|线性层（输入256→输出256），无激活|256×256=65536|输出256维高阶表征，供双网络调用|
|总计|-|7040+24576+32768+98304+65536=228224（≈22.8万）|-|
优化措施：采用1×1卷积降低计算量，批量归一化提升训练稳定性，所有层使用PyTorch量化（FP16）减少推理延迟。

#### 3.1.3 感知层与双网络的联合训练机制

感知层并非独立训练模块，而是作为行为网络、价值网络的前置特征提取组件，与双网络构成“感知-决策”一体化训练链路，通过联合训练优化整体表征提取与决策能力，具体设计如下：

##### 3.1.3.1 联合训练核心逻辑

1.  链路结构：感知层的256维高阶表征直接作为行为网络、价值网络的输入，三者共享前向传播链路，形成“原始局面→感知层编码→双网络决策”的端到端训练范式；

2.  梯度传递：以双网络的训练目标为核心优化方向，训练时将双网络的损失（行为网络交叉熵损失+价值网络MSE损失）反向传播至感知层，同步更新感知层的卷积核、线性层等参数，确保感知层生成的高阶表征能精准适配双网络的决策需求；

3.  参数协同：感知层与双网络使用统一的优化器（AdamW）和训练超参数（批次大小、学习率等），避免独立训练导致的表征与决策目标错位，保障参数更新的协同性。

##### 3.1.3.2 联合训练实施流程（覆盖预训练+RL微调全阶段）

1. 预训练阶段联合优化：

2. 数据输入：将预处理后的贝叶斯AI轨迹数据（含56维原始特征）输入感知层；

3. 前向传播：感知层输出256维高阶表征，分别传入行为网络和价值网络，生成动作预测、胜率与方差预测结果；

4. 损失计算：计算行为网络“交叉熵损失+方差惩罚项”、价值网络“综合价值MSE+方差MSE”，将两类损失加权求和（行为网络损失权重0.6，价值网络损失权重0.4）得到总损失；

5. 梯度更新：总损失反向传播至感知层、行为网络、价值网络，同步更新三者参数，完成一轮训练。

6. RL微调阶段联合优化：

7. 自对弈交互：MCTS调用联合训练后的“感知层+双网络”与PoolTool环境交互，生成含多轨迹扰动的自对弈轨迹；

8. 轨迹采样：从自对弈轨迹池中采样局面-动作-奖励数据，作为微调输入；

9. 损失适配：在原有总损失基础上，加入策略梯度损失（基于MCTS访问计数的动作权重），强化高价值动作的学习；

10. 迭代更新：保持感知层与双网络的联合链路，使用微调学习率（1e-5）迭代更新参数，确保环境交互过程中感知层能持续适配双网络的决策优化需求。

##### 3.1.3.3 训练保障措施

- 参数初始化：感知层采用He初始化，双网络采用Xavier初始化，避免训练初期梯度消失/爆炸，保障联合训练收敛性；

- 学习率调度：预训练阶段三者使用相同学习率（1e-4），RL微调阶段同步降至1e-5，通过StepLR调度器（每20epoch衰减0.9）进一步稳定训练；

- 正则化协同：感知层与双网络均加入L2正则化（权重衰减系数1e-5），避免过拟合，同时在感知层的批量归一化层中加入0.1的dropout概率，增强表征的泛化能力；

- 训练监控：实时跟踪感知层输出表征的方差（确保表征区分度）、双网络的验证集损失，当表征方差低于0.01或双网络损失收敛时，调整感知层的训练权重，避免过度拟合。

### 3.2 行为网络设计（轻量精准，抗随机过滤）

#### 3.2.1 核心目标

生成合法、高质量、低波动的候选动作，输出动作先验概率分布，过滤高风险随机动作。

#### 3.2.2 网络结构（精确参数，总参≈1400万）

|模块|结构配置|参数规模|核心作用|
|---|---|---|---|
|输入层|接收感知层256维高阶表征|0|特征输入|
|主干网络（残差块6层）|残差块1-2：1×1卷积（256→256）+ ReLU + BN；残差块3-4：1×1卷积（256→512）+ ReLU + BN；残差块5-6：1×1卷积（512→256）+ ReLU + BN；均含shortcut连接|残差块1-2：(256×256×2)×2=262144；残差块3-4：(256×512+512×512)×2=786432；残差块5-6：(512×256+256×256)×2=393216；合计：262144+786432+393216=1,441,792（≈144万）|动作特征深度提取|
|输出层（3分支）|1. 角度分支：线性层（256→2），输出高斯分布均值+方差（0-360°）；2. 力度分支：线性层（256→2），输出高斯分布均值+方差（0-1）；3. 击球点分支：线性层（256→4），输出x/y方向高斯分布均值+方差（母球区域）|角度分支：256×2=512；力度分支：256×2=512；击球点分支：256×4=1024；合计：2048|输出连续动作分布，含不确定性信息|
|动作过滤模块（非网络层）|基于贝叶斯AI规则掩码，过滤非法动作；计算动作方差阈值（方差>0.2视为高波动，过滤）|0（逻辑模块）|输出8-10个低风险合法候选动作|
|总计|-|1,441,792 + 2,048 = 1,443,840（≈144万）|-|
#### 3.2.3 关键配置

- 动作离散化：角度分16档（每22.5°一档）、力度分8档（每12.5%一档）、击球点分4档，平衡精度与搜索效率

- 损失函数：交叉熵损失（预测分布 vs 贝叶斯AI赢家动作分布） + 0.1×方差惩罚项（抑制高波动动作）

- 优化器：AdamW，预训练学习率1e-4，RL微调学习率1e-5，批次大小256

### 3.3 价值网络设计（含不确定性建模，总参≈600万）

#### 3.3.1 核心目标

精准预测局面综合价值（胜率），同时输出价值方差（不确定性），适配己方视角，为MCTS提供抗随机决策依据。

#### 3.3.2 网络结构（精确参数）

|模块|结构配置|参数规模|核心作用|
|---|---|---|---|
|输入层|接收感知层256维高阶表征 + 2维己方视角特征（目标球类型+球权）|0|融合局面特征与己方视角|
|主干网络（残差块4层）|残差块1-2：1×1卷积（258→256）+ ReLU + BN；残差块3-4：1×1卷积（256→128）+ ReLU + BN；均含shortcut连接|残差块1-2：(258×256+256×256)×2=260,096；残差块3-4：(256×128+128×128)×2=98,304；合计：260,096+98,304=358,400（≈35.8万）|价值特征提取|
|输出层（4分支+融合）|1. 胜率分支：线性层（128→1）+ Sigmoid（输出0-1）；2. 剩余杆数分支：线性层（128→1）（输出0-60）；3. 犯规风险分支：线性层（128→1）+ Sigmoid（输出0-1）；4. 方差分支：线性层（128→1）+ Softplus（输出≥0，表征不确定性）；5. 综合价值融合：α×胜率 + β×(1/杆数) - γ×犯规风险（α=0.6, β=0.3, γ=0.1），Tanh缩放到[-1,1]|胜率分支：128×1=128；杆数分支：128×1=128；风险分支：128×1=128；方差分支：128×1=128；合计：512|输出多维度价值信息，含不确定性|
|总计|-|358,400 + 512 = 358,912（≈35.9万）|-|
补充说明：价值网络总参约35.9万，加上感知层22.8万，双网络+感知层总参≈58.7万，远低于2000万约束，预留足够冗余。

#### 3.3.3 关键配置

- 损失函数：MSE（预测综合价值 vs 贝叶斯AI胜率+人类标注杆数） + 0.05×方差MSE（预测方差 vs 实际多轨迹方差）

- 训练数据：含多轨迹扰动数据，增强不确定性建模能力

- 优化器：AdamW，预训练学习率1e-4，RL微调学习率1e-5，批次大小256

### 3.4 抗随机MCTS决策系统设计（核心模块）

#### 3.4.1 核心设计思路

通过“多轨迹模拟、方差惩罚UCB、动态搜索参数”三重机制，抵消环境随机性对决策的影响，同时保障实时性。

#### 3.4.2 关键参数（适配随机性+实时性）

|参数|初始值（训练初期）|训练后期值（收敛阶段）|说明|
|---|---|---|---|
|探索系数c|1.5|0.5|初期多探索，后期多利用，平衡策略多样性与稳定性|
|最大搜索步数|30|50|控制单次决策时间，30步≈500ms，50步≈800ms|
|候选动作数|8|10|行为网络生成的高概率低波动动作数|
|多轨迹模拟次数k|3|2|同一动作的模拟次数，取均值降低随机偏差，k越大越稳健但耗时越长|
|方差惩罚权重λ|0.3|0.1|UCB公式中方差惩罚的权重，初期强化稳健性，后期平衡收益|
|折扣因子γ|0.99|0.99|未来奖励的权重，固定值保障长程策略一致性|
|关键局面阈值|0.7|0.7|价值网络输出风险值>0.7为关键局面，自动提升k=5、搜索步数=50|
#### 3.4.3 核心流程（Numba加速版）

1. **选择（Selection）**：基于改进的方差惩罚UCB公式选择最优路径，公式如下： $UCB(s,a) = Q(s,a) + c \times P(s,a) \times \frac{\sqrt{\sum N(s,·)}}{1 + N(s,a)} - \lambda \times \sigma(s,a)$ 其中：Q(s,a)为价值网络输出的综合价值均值；P(s,a)为行为网络输出的先验概率；N(s,a)为动作a的访问次数；ΣN(s,·)为父节点总访问次数；σ(s,a)为价值网络输出的方差（不确定性）；λ为方差惩罚权重。

2. **扩展（Expansion）**：为叶子节点生成候选动作对应的子节点，初始化Q/N/σ值，直接剪枝非法动作和高方差动作（σ>0.2）。

3. **模拟（Simulation）**：多轨迹模拟机制——对每个扩展节点，执行k次相同动作，得到k个奖励z1-z3，取均值z_avg作为模拟结果；模拟过程中用行为网络选高概率动作，关键局面用价值网络直接预测结果，无需模拟到终局。

4. **回溯（Backpropagation）**：用Numba加速回溯逻辑，沿路径更新所有节点的Q（均值）、N（访问次数）、σ（方差）；注意切换己方视角，奖励值随球权取反。

#### 3.4.4 性能优化措施

- 核心函数加速：UCB计算、回溯更新等核心函数用Numba的@njit装饰器编译，提升效率10-20倍。

- 节点存储优化：用NumPy数组替代Python类存储节点信息（Q/N/σ/P），降低内存开销50%以上。

- 并行模拟：多轨迹模拟采用多线程并行执行，k=3时耗时降低60%（用Python concurrent.futures实现）。

- 动态剪枝：对访问次数多且Q值低的节点，后续搜索中降低选择优先级，减少无效搜索。

### 3.5 基于PoolTool的随机仿真环境设计

#### 3.5.1 核心接口（标准化Gym接口）

|接口|输入|输出|核心逻辑|
|---|---|---|---|
|reset()|无|初始局面（球坐标/球权/剩余杆数/已进袋列表，适配PoolTool格式）|调用PoolTool初始化台球桌与球布局，生成随机合法开球局面，初始化环境状态|
|step(action)|击球动作（角度/力度/击球点，转换为PoolTool可识别格式）|新局面（PoolTool输出转换后）、即时奖励、是否终局、规则信息（犯规/换权/随机偏差）|调用PoolTool执行动作，添加±0.5-1cm随机偏差模拟物理扰动，基于PoolTool结果判定规则，计算奖励|
|get_legal_actions()|当前局面（PoolTool格式）|合法动作列表（过滤非法动作，转换为决策层格式）|基于台球规则与PoolTool环境约束，生成合法动作掩码|
|multi_step(action, k=3)|击球动作、模拟次数k|k个新局面、k个奖励、方差统计|基于PoolTool多次执行同一动作，返回统计结果，供MCTS抵消随机性使用|
|render()|渲染模式（文本/图形）|局面可视化结果|文本模式输出标准化球坐标，图形模式调用PoolTool可视化接口绘制球桌与球布局|
#### 3.5.2 规则引擎（核心，适配随机性）

- 合法击球判定：白球先碰己方目标球；无进球时至少一颗球碰库；击球点在母球合法区域。

- 犯规判定：白球进袋、先碰对方球、未碰库（无进球）、击球点非法、力度过大导致白球出界。

- 终局判定：清己方球+合法打黑8（胜）；超时（60杆）、犯规判负、黑8提前进袋（负）。

- 随机偏差模型：基于真实台球物理规律，通过PoolTool的物理参数扰动接口，在动作执行后对所有球的坐标添加±0.5-1cm的高斯分布随机偏差，模拟桌面摩擦、球的旋转误差等随机性，偏差参数可通过PoolTool接口动态调整。

#### 3.5.3 奖励函数（含稳健性奖励）

```python

def calculate_reward(is_legal, is_goal, goal_type, is_foul, remaining_rods, action_variance):
    # 基础奖励
    reward = 0.0
    if is_legal:
        reward += 0.1  # 合法击球基础奖励
    if is_goal and goal_type == "self":
        reward += 1.0  # 进己方球奖励
    if is_goal and goal_type == "black8" and self_balls_clear:
        reward += 10.0  # 合法打黑8胜利奖励
    # 惩罚项
    if is_foul:
        reward -= 5.0  # 犯规惩罚
    if remaining_rods < 10:
        reward -= 0.5  # 超时风险惩罚
    # 稳健性奖励（抑制高波动动作）
    reward += 0.2 * (1 - action_variance)  # 动作方差越小，奖励越高
    return reward
```

## 四、训练流程设计（4周开发+训练计划，含抗随机训练）

### 4.1 阶段划分与时间规划

|阶段|时间|核心任务|负责人|关键交付物|验证指标|
|---|---|---|---|---|---|
|阶段1：环境与数据准备|第1周（7天）|1. 基于PoolTool开发随机仿真环境（含标准化接口、规则引擎、随机偏差扰动，适配贝叶斯AI交互）；2. 采集贝叶斯AI轨迹（10万+条）；3. 人类标注典型局面（1000+个）；4. 数据增强（添加随机扰动、轨迹扩充）|仿真环境开发+模型开发2|PoolTool封装环境、预训练数据集（贝叶斯AI轨迹+增强数据+人类标注）、环境测试用例|环境规则判定准确率≥99%；预训练数据量≥10万条；数据增强后数据量≥100万条|
|阶段2：双网络预训练（含不确定性建模）|第2周（7天）|1. 开发感知层+行为网络+价值网络代码；2. 行为网络预训练（模仿贝叶斯AI赢家动作，含方差惩罚）；3. 价值网络预训练（对齐贝叶斯AI胜率，含不确定性校准）；4. 预训练模型验证与调优|模型开发1+模型开发2+算法架构师|双网络训练代码、预训练权重、预训练日志|行为网络：动作预测准确率≥80%，非法动作率≤3%；价值网络：胜率预测MSE≤0.01，方差预测偏差≤0.05|
|阶段3：MCTS集成+RL微调（抗随机训练）|第3周（7天）|1. 开发抗随机MCTS核心代码（Numba加速）；2. 集成MCTS与双网络；3. 自对弈训练（10万轮，含多轨迹数据）；4. 双网络迭代更新（含不确定性校准）；5. 实时性优化（Numba/并行模拟）|算法架构师+模型开发1+模型开发2|集成后的决策系统、RL训练脚本、微调后权重、自对弈轨迹池|对贝叶斯AI胜率≥50%；单次决策时间≤1s；策略稳健性≥75%|
|阶段4：优化与验证|第4周（7天）|1. 性能优化（MCTS参数调优、模型量化）；2. 对抗测试（vs贝叶斯AI/带随机性的对手）；3. 稳健性测试（相同局面多轮决策一致性）；4. 规则适配性测试；5. 撰写测试报告与部署指南|测试验证工程师+全员协作|最终AI模型、测试报告、部署指南、可视化分析结果|对贝叶斯AI胜率≥70%；单次决策≤1s；非法动作率≤5%；策略稳健性≥85%|
### 4.2 预训练细节（阶段2）

- 数据集：贝叶斯AI轨迹（10万条）+ 人类标注局面（1000个），经数据增强（±1cm随机扰动、轨迹片段重组）生成100万条训练数据，按8:2划分训练集/验证集。其中贝叶斯AI轨迹通过Agent与PoolTool环境交互生成，具体流程见3.5.4训练数据生成详细说明。

- 行为网络预训练：
       

    - 标签：贝叶斯AI赢家动作分布（角度/力度/击球点）+ 非法动作掩码（负样本）；优先选用贝叶斯AI对战中获胜方的动作作为正样本，提升标签质量。

    - 损失函数：交叉熵损失 + 0.1×方差惩罚项。

    - 训练配置：AdamW优化器，学习率1e-4，批次大小256，训练轮数100epoch，早停机制（验证集损失3轮不下降则停止）。

- 价值网络预训练：
        

    - 标签：贝叶斯AI对战输出的胜率 + 人类标注的剩余杆数 + 多轨迹模拟得到的方差标签；基于贝叶斯AI的胜率预测结果，保障标签可靠性。

    - 损失函数：MSE（综合价值） + 0.05×方差MSE。

    - 训练配置：AdamW优化器，学习率1e-4，批次大小256，训练轮数100epoch，早停机制。

### 4.3 RL微调细节（阶段3，含抗随机训练）

#### 4.3.1 自对弈闭环流程

1. 初始化：加载预训练的双网络权重，初始化MCTS参数（c=1.5，k=3）。

2. 自对弈生成轨迹：
        
> （注：文档部分内容可能由 AI 生成）